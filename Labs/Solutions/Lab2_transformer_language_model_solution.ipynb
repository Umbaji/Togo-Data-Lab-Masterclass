{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> üìå **Avant de commencer :**\n",
        ">\n",
        "> [![Ouvrir dans Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RoYOImTMfhTV9iPxXop4CrW9scX_Dm3g?usp=sharing)\n",
        ">\n",
        "> üîÅ *Pensez √† faire une copie dans votre Google Drive (`Fichier > Enregistrer une copie dans Drive`) pour pouvoir l‚Äô√©diter librement.*\n"
      ],
      "metadata": {
        "id": "JHsIe78T-SoX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU0wD0LEBbVZ"
      },
      "source": [
        "# Mod√®le linguistique avec les Transformers\n",
        "\n",
        "### T√¢che\n",
        "\n",
        "L'objectif est d'apprendre √† pr√©dire le mot suivant √† partir d'une s√©quence d'entr√©e √† l'aide d'un mod√®le de langage de type Transformer.\n",
        "\n",
        "Le jeu de donn√©es utilis√© est PTB (Penn Treebank)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "def download_github_files(repo_url, folder_path, output_dir=\"data\"):\n",
        "    \"\"\"\n",
        "    T√©l√©charger tous les fichiers d'un dossier sp√©cifique sur GitHub.\n",
        "\n",
        "    Arguments :\n",
        "        repo_url (str) : L'URL du d√©p√¥t GitHub (ex. : \"https://github.com/user/repo\").\n",
        "        folder_path (str) : Le chemin vers le dossier dans le d√©p√¥t (ex. : \"notebooks/demo\").\n",
        "        output_dir (str) : R√©pertoire local pour enregistrer les fichiers t√©l√©charg√©s (par d√©faut : \"data\").\n",
        "    \"\"\"\n",
        "    # V√©rifie que l'URL est bien celle d‚Äôun d√©p√¥t GitHub\n",
        "    if \"github.com\" not in repo_url:\n",
        "        raise ValueError(\"URL GitHub invalide.\")\n",
        "\n",
        "    # Construit l‚ÄôURL vers l‚ÄôAPI GitHub pour acc√©der aux contenus bruts\n",
        "    repo_parts = repo_url.rstrip(\"/\").split(\"/\")\n",
        "    user, repo = repo_parts[-2], repo_parts[-1]\n",
        "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{folder_path}\"\n",
        "\n",
        "    # R√©cup√®re le contenu du dossier depuis l‚ÄôAPI GitHub\n",
        "    response = requests.get(api_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"√âchec lors de la r√©cup√©ration du contenu du dossier. Code HTTP : {response.status_code}\")\n",
        "\n",
        "    files = response.json()\n",
        "\n",
        "    # Cr√©e le r√©pertoire de sortie s'il n‚Äôexiste pas\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # T√©l√©charge chaque fichier\n",
        "    for file in files:\n",
        "        if file[\"type\"] == \"file\":\n",
        "            file_url = file[\"download_url\"]\n",
        "            file_name = file[\"name\"]\n",
        "            file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "            print(f\"T√©l√©chargement de {file_name}...\")\n",
        "            file_response = requests.get(file_url)\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(file_response.content)\n",
        "            print(f\"{file_name} enregistr√© dans le dossier {output_dir}\")\n",
        "\n",
        "    print(\"Tous les fichiers ont √©t√© t√©l√©charg√©s avec succ√®s.\")\n",
        "\n",
        "\n",
        "repo_url = \"https://github.com/atouammi/basemodel\"\n",
        "folder_path = \"data/ptb\"\n",
        "download_github_files(repo_url, folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg4V3UqDAtX2",
        "outputId": "370afd96-67ec-43d6-c2ec-fbbd9f9a3842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T√©l√©chargement de generate_ptb.ipynb...\n",
            "generate_ptb.ipynb enregistr√© dans le dossier data\n",
            "T√©l√©chargement de idx2word.pt...\n",
            "idx2word.pt enregistr√© dans le dossier data\n",
            "T√©l√©chargement de test_data.pt...\n",
            "test_data.pt enregistr√© dans le dossier data\n",
            "T√©l√©chargement de train_data.pt...\n",
            "train_data.pt enregistr√© dans le dossier data\n",
            "T√©l√©chargement de word2idx.pt...\n",
            "word2idx.pt enregistr√© dans le dossier data\n",
            "Tous les fichiers ont √©t√© t√©l√©charg√©s avec succ√®s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls data"
      ],
      "metadata": {
        "id": "S531fhhpAtXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5f9b0f-717d-4a17-f7d9-fef15af1741b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate_ptb.ipynb  idx2word.pt  test_data.pt  train_data.pt  word2idx.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import os.path\n",
        "\n",
        "\n",
        "path_data = 'data/'\n",
        "# _ = check_ptb_dataset_exists(path_data)\n",
        "word2idx  =  torch.load(path_data + '/word2idx.pt')\n",
        "idx2word  =  torch.load(path_data + '/idx2word.pt')\n",
        "\n",
        "\n",
        "def normalize_gradient(net):\n",
        "\n",
        "    grad_norm_sq=0\n",
        "\n",
        "    for p in net.parameters():\n",
        "        # grad_norm_sq += p.grad.data.norm()**2\n",
        "        if p.grad is not None:\n",
        "            grad_norm_sq += p.grad.data.norm()**2\n",
        "\n",
        "    grad_norm=math.sqrt(grad_norm_sq)\n",
        "\n",
        "    if grad_norm<1e-4:\n",
        "        net.zero_grad()\n",
        "        print('La norme du gradient est proche de z√©ro')\n",
        "    else:\n",
        "        for p in net.parameters():\n",
        "            if p.grad is not None:\n",
        "                p.grad.data.div_(grad_norm)\n",
        "\n",
        "    return grad_norm\n",
        "\n",
        "\n",
        "def display_num_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    print('Ce r√©seau de neurones contient {} param√®tres (soit {:.2f} millions)'.format(\n",
        "        nb_param, nb_param/1e6)\n",
        "         )\n",
        "\n",
        "def sentence2vector(sentence):\n",
        "    words = sentence.split()\n",
        "    x = torch.LongTensor(len(words),1)\n",
        "    for idx, word in enumerate(words):\n",
        "\n",
        "         if word not in word2idx:\n",
        "            print('Vous avez entr√© un mot qui n\\'est pas dans le vocabulaire.')\n",
        "            print('Assurez-vous qu\\'il n\\'y a pas de lettres majuscules.')\n",
        "         else:\n",
        "            x[idx,0]=word2idx[word]\n",
        "    return x\n",
        "\n",
        "\n",
        "def show_next_word(scores):\n",
        "    num_word_display=30\n",
        "    prob=F.softmax(scores,dim=2)\n",
        "    p=prob[-1].squeeze()\n",
        "    p,word_idx = torch.topk(p,num_word_display)\n",
        "\n",
        "    for i,idx in enumerate(word_idx):\n",
        "        percentage= p[i].item()*100\n",
        "        word=  idx2word[idx.item()]\n",
        "        print(\"{:.1f}%\\t\".format(percentage), word)\n"
      ],
      "metadata": {
        "id": "D3fdc86j1g0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3yiLJBBBbVh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import time\n",
        "#import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe01LtUIBbVj"
      },
      "source": [
        "### GPU\n",
        "\n",
        "Il est recommand√© d'ex√©cuter ce code sur un GPU :  \n",
        "* Temps pour 1 √©poque sur GPU : 48 secondes avec Google Colab Tesla P100-PCIE-16GB  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_A4HBT3BbVj",
        "outputId": "6076943d-accd-48bf-a042-915dcd1989a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilisation du p√©riph√©rique: cuda\n"
          ]
        }
      ],
      "source": [
        "# V√©rifier si le GPU est disponible\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilisation du p√©riph√©rique: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S3W2p84BbVk"
      },
      "source": [
        "### T√©l√©chargement de Penn Tree Bank\n",
        "\n",
        "Le tenseur `train_data` est compos√© de 20 colonnes contenant 46 479 mots.  \n",
        "Le tenseur `test_data` est compos√© de 20 colonnes contenant 4 121 mots.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnE-JNp_BbVk",
        "outputId": "b54f6d2c-2b82-4c74-a7e9-dfefc7192ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([46479, 20])\n",
            "torch.Size([4121, 20])\n"
          ]
        }
      ],
      "source": [
        "#from utils import check_ptb_dataset_exists\n",
        "#data_path=check_ptb_dataset_exists()\n",
        "\n",
        "train_data  =  torch.load(path_data+'/train_data.pt')\n",
        "test_data   =  torch.load(path_data+'/test_data.pt')\n",
        "\n",
        "print(  train_data.size()  )\n",
        "print(  test_data.size()   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gpJiycm1fXI",
        "outputId": "14cfc51b-9711-44a6-e3f5-5f2685893d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDxTJimKBbVl"
      },
      "source": [
        "### Quelques constantes associ√©es avec le jeu de donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS9EUUvbBbVm"
      },
      "outputs": [],
      "source": [
        "bs = 20\n",
        "vocab_size = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U9UvE28BbVm"
      },
      "source": [
        "### Cr√©er une classe de r√©seau √† m√©canisme d'attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRmg_OUaBbVn"
      },
      "outputs": [],
      "source": [
        "def generate_positional_encoding(seq_length, dim):\n",
        "    assert dim == 2* (dim//2)  # v√©rifier si la dimension est divisible par 2\n",
        "    pe = torch.zeros(seq_length, dim)\n",
        "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
        "    pe[:,0::2] = torch.sin(position * div_term)\n",
        "    pe[:,1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, d, d_head, dropout):\n",
        "        super().__init__()\n",
        "        self.LN_MHA = nn.LayerNorm(d_head)\n",
        "        self.LN_MLP = nn.LayerNorm(d_head)\n",
        "        self.query = nn.Linear(d, d_head, bias=False)  # couche d'embedding pour la requ√™te\n",
        "        self.key = nn.Linear(d, d_head, bias=False)     # couche d'embedding pour la cl√©\n",
        "        self.value = nn.Linear(d, d_head)               # couche d'embedding pour la valeur\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, H):  # taille(H) = [batch_size, seq_length, d]\n",
        "        batch_size = H.size(0); batch_len = H.size(1)\n",
        "        H_in = H  # Ajouter une connexion r√©siduelle (RC)\n",
        "        # Calcul d'une t√™te d'attention unique : H = Softmax( QK^T / ‚àöd ) V\n",
        "        Q = self.query(H)  # taille = [batch_size, batch_length, d]\n",
        "        K = self.key(H)    # taille = [batch_size, batch_length, d]\n",
        "        V = self.value(H)  # taille = [batch_size, batch_length, d]\n",
        "        attention_score = Q @ K.transpose(2,1) * Q.size(2)**-0.5  # QK^T/‚àöd, (B,L,d) @ (B,d,L) => (B,L,L)\n",
        "        mask = torch.tril(torch.ones(batch_len,batch_len)).long().to(device)  # masque pour n'utiliser que les tokens pr√©c√©dents : { token(‚â§t) }, taille = [batch_len, batch_len]\n",
        "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf'))  # softmax(-inf)=0 emp√™che l'utilisation des tokens suivants pour la pr√©diction\n",
        "        attention_score = torch.softmax(attention_score, dim=2)  # somme des poids = 1\n",
        "        attention_score = self.dropout(attention_score)  # appliquer dropout sur les scores d'attention\n",
        "        H_HA = attention_score @ V  # softmax( QK^T / ‚àöd ) V, (B,L,L) @ (B,L,d) => (B,L,d)\n",
        "        return H_HA  # retourne les scores de pr√©diction pour le prochain token\n",
        "\n",
        "class MultipleAttentionHead(nn.Module):\n",
        "    def __init__(self, d, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        d_head = d // num_heads  # dim_head = d // num_heads, g√©n√©ralement 64\n",
        "        assert d == d_head * num_heads  # v√©rifier la divisibilit√©\n",
        "        self.MHA = nn.ModuleList([ AttentionHead(d, d_head, dropout) for _ in range(num_heads) ])\n",
        "        self.WO = nn.Linear(d, d)  # couche de combinaison\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, H):  # taille(H) = [batch_size, seq_length, d]\n",
        "        batch_size = H.size(0); seq_length = H.size(1)\n",
        "        H_heads = []\n",
        "        for HA_layer in self.MHA:\n",
        "            H_heads.append(HA_layer(H))  # taille = [batch_size, seq_length, d_head]\n",
        "        H_heads = torch.cat(H_heads, dim=2)  # taille = [batch_size, seq_length, d]\n",
        "        H_heads = self.dropout(H_heads)  # appliquer dropout sur les activations d'attention\n",
        "        H = self.WO(H_heads)  # taille = [batch_size, seq_length, d]\n",
        "        return H\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.LN_MHA = nn.LayerNorm(d)\n",
        "        self.LN_MLP = nn.LayerNorm(d)\n",
        "        self.MHA = MultipleAttentionHead(d, num_heads, dropout)\n",
        "        self.MLP = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d, d))\n",
        "    def forward(self, H):  # taille = [batch_size, seq_length, d]\n",
        "        # T√™tes d'attention multiples avec normalisation couche (LN) et connexion r√©siduelle (RC)\n",
        "        H = H + self.MHA(self.LN_MHA(H))  # taille = [batch_size, seq_length, d]\n",
        "        # MLP avec normalisation couche (LN) et connexion r√©siduelle (RC)\n",
        "        H = H + self.MLP(self.LN_MLP(H))  # taille = [batch_size, seq_length, d]\n",
        "        return H  # taille = [batch_size, seq_length, d]\n",
        "\n",
        "class Transformer_decoder(nn.Module):\n",
        "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
        "        super().__init__()\n",
        "        self.TR_Blocks = nn.ModuleList([ TransformerBlock(d, num_heads, dropout) for _ in range(num_blocks) ])\n",
        "    def forward(self, batch_seq, pos_enc):\n",
        "        H = batch_seq.transpose(1,0)  # taille = [batch_size, seq_length, d]\n",
        "        batch_size = H.size(0); batch_len = H.size(1)\n",
        "        # Ajouter l'encodage positionnel\n",
        "        pos_enc = pos_enc.unsqueeze(dim=0)  # taille = [1, seq_length, d]\n",
        "        H = H + pos_enc                    # taille = [batch_size, seq_length, d]\n",
        "        # Appliquer les blocs transformer\n",
        "        for TR_Block in self.TR_Blocks:\n",
        "            H = TR_Block(H)\n",
        "        # Sortie\n",
        "        H = H.permute(1,0,2)  # taille = [batch_length, batch_size, d]\n",
        "        return H  # retourne les scores de pr√©diction pour le prochain token\n",
        "\n",
        "class ANN(nn.Module):\n",
        "\n",
        "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
        "        super(ANN, self).__init__()\n",
        "        self.decoder = Transformer_decoder(d, num_heads, num_blocks, seq_length, dropout)\n",
        "\n",
        "    def forward(self, g_seq , pos ):\n",
        "        h_dec_seq = self.decoder( g_seq , pos )\n",
        "        return h_dec_seq\n",
        "\n",
        "class attention_net(nn.Module):\n",
        "\n",
        "    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n",
        "        super(attention_net, self).__init__()\n",
        "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
        "        self.layer2 = ANN(d, num_heads, num_blocks, seq_length, dropout)\n",
        "        self.layer3 = nn.Linear( hidden_size , vocab_size )\n",
        "\n",
        "    def forward(self, word_seq, pos ):\n",
        "        g_seq     =   self.layer1( word_seq )  # taille = (seq_length, batch_size, hidden_dim)\n",
        "        h_seq     =   self.layer2( g_seq , pos )  # taille = (seq_length, batch_size, hidden_dim)\n",
        "        score_seq =   self.layer3( h_seq )  # taille = (seq_length, batch_size, vocab_size)\n",
        "        return score_seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4FH3SgyBbVt"
      },
      "source": [
        "### Fonction pour √©valuer le r√©seau sur un ensemble de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcOReFN0IHcP"
      },
      "outputs": [],
      "source": [
        "def eval_on_test_set():\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    running_loss=0\n",
        "    num_batches=0\n",
        "\n",
        "    for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
        "\n",
        "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
        "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
        "        pos = generate_positional_encoding(seq_length, hidden_size)\n",
        "\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        pos = pos.to(device)\n",
        "\n",
        "        scores = net( minibatch_data, pos )\n",
        "\n",
        "        minibatch_label = minibatch_label.view(  bs*seq_length )\n",
        "        scores = scores.view(  bs*seq_length , vocab_size)\n",
        "\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    total_loss = running_loss/num_batches\n",
        "    print('test: exp(loss) = ', math.exp(total_loss)  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gnTKxy9BbVr"
      },
      "source": [
        "### Construire le r√©seau. Choisir une taille cach√©e de 128, le nombre de t√™tes √† 4 et le nombre de blocs √† 2.\n",
        "### Combien de param√®tres au total ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADOYeTS6_SLO",
        "outputId": "99bda85b-f297-4ac9-d28b-d2b96ccf1ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_net(\n",
            "  (layer1): Embedding(10000, 128)\n",
            "  (layer2): ANN(\n",
            "    (decoder): Transformer_decoder(\n",
            "      (TR_Blocks): ModuleList(\n",
            "        (0-1): 2 x TransformerBlock(\n",
            "          (LN_MHA): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (LN_MLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (MHA): MultipleAttentionHead(\n",
            "            (MHA): ModuleList(\n",
            "              (0-3): 4 x AttentionHead(\n",
            "                (LN_MHA): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "                (LN_MLP): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "                (query): Linear(in_features=128, out_features=32, bias=False)\n",
            "                (key): Linear(in_features=128, out_features=32, bias=False)\n",
            "                (value): Linear(in_features=128, out_features=32, bias=True)\n",
            "                (dropout): Dropout(p=0.95, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (WO): Linear(in_features=128, out_features=128, bias=True)\n",
            "            (dropout): Dropout(p=0.95, inplace=False)\n",
            "          )\n",
            "          (MLP): Sequential(\n",
            "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Dropout(p=0.95, inplace=False)\n",
            "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layer3): Linear(in_features=128, out_features=10000, bias=True)\n",
            ")\n",
            "Ce r√©seau de neurones contient 2967056 param√®tres (soit 2.97 millions)\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "num_heads = 4\n",
        "num_blocks = 2\n",
        "dropout = 0.95\n",
        "seq_length = 100\n",
        "\n",
        "net = attention_net(hidden_size, num_heads, num_blocks, seq_length, dropout)\n",
        "print(net)\n",
        "display_num_param(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khaDFxmwBbVr"
      },
      "source": [
        "### Transf√©rer le r√©seau sur le GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp63FT_v_SLQ"
      },
      "outputs": [],
      "source": [
        "net = net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecX1xVaBbVs"
      },
      "source": [
        "### Choisir la fonction de perte comme l'entropie crois√©e et l'optimiseur comme Adam, ainsi que les hyperparam√®tres importants suivants :\n",
        "\n",
        "* initial learning rate = 0.001\n",
        "* sequence length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBm4Aw_j_SLR"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "my_lr = 0.001\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n",
        "\n",
        "pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frtpNDBLBbVs"
      },
      "source": [
        "### Faire 5 passages sur l‚Äôensemble d‚Äôentra√Ænement  \n",
        "### Observer la perplexit√© sur l‚Äôensemble d‚Äôentra√Ænement et sur l‚Äôensemble de test  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McQc1_xkBbVp",
        "outputId": "b717a451-e8fe-404b-89cf-e310fb24464f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "√©poque = 0 \t temps = 9.094287633895874 \t lr = 0.001 \t exp(loss) = 680.3077307862844\n",
            "test: exp(loss) =  365.23720805758177\n",
            "\n",
            "√©poque = 1 \t temps = 16.386220455169678 \t lr = 0.001 \t exp(loss) = 294.13487818708074\n",
            "test: exp(loss) =  257.01290844500113\n",
            "\n",
            "√©poque = 2 \t temps = 24.889225959777832 \t lr = 0.0009090909090909091 \t exp(loss) = 207.11752464353142\n",
            "test: exp(loss) =  214.31581592394815\n",
            "\n",
            "√©poque = 3 \t temps = 32.57538723945618 \t lr = 0.0008264462809917355 \t exp(loss) = 161.85380147916237\n",
            "test: exp(loss) =  192.47670005076125\n",
            "\n",
            "√©poque = 4 \t temps = 40.2217652797699 \t lr = 0.0007513148009015777 \t exp(loss) = 134.0087874013894\n",
            "test: exp(loss) =  179.95568609705285\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "start=time.time()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # diviser le taux d'apprentissage par 1.1 sauf apr√®s la premi√®re √©poque\n",
        "    if epoch >= 2:\n",
        "        optimizer.param_groups[0]['lr'] /= 1.1\n",
        "        my_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # initialiser les quantit√©s cumul√©es √† z√©ro au d√©but de l'√©poque\n",
        "    running_loss=0\n",
        "    num_batches=0\n",
        "    for count in range(0, 46478 - seq_length, seq_length):\n",
        "\n",
        "        # Remettre les gradients √† z√©ro\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # cr√©er un mini-batch et l'encodage positionnel\n",
        "        minibatch_data = train_data[count : count + seq_length]\n",
        "        minibatch_label = train_data[count + 1 : count + seq_length + 1]\n",
        "        pos = generate_positional_encoding(seq_length, hidden_size)  # taille=(seq_length, hidden_dim)\n",
        "\n",
        "        # envoyer les tenseurs sur le GPU\n",
        "        minibatch_data = minibatch_data.to(device)\n",
        "        minibatch_label = minibatch_label.to(device)\n",
        "        pos = pos.to(device)\n",
        "\n",
        "        # passage avant du mini-batch dans le r√©seau\n",
        "        scores = net(minibatch_data, pos)  # taille=(seq_length, bs, vocab_size)\n",
        "\n",
        "        # remodeler scores et labels en un grand batch de taille bs*seq_length\n",
        "        scores = scores.view(bs * seq_length, vocab_size)  # taille=(bs*seq_length, vocab_size)\n",
        "        minibatch_label = minibatch_label.view(bs * seq_length)  # taille=(bs*seq_length)\n",
        "\n",
        "        # calculer la moyenne des pertes sur ce grand batch\n",
        "        loss = criterion(scores, minibatch_label)\n",
        "\n",
        "        # r√©tropropagation pour calculer dL/dR, dL/dV et dL/dW\n",
        "        loss.backward()\n",
        "\n",
        "        # faire un pas de descente de gradient stochastique : R = R - lr*(dL/dR), etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # mettre √† jour la perte cumul√©e\n",
        "        running_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    # calculer les statistiques pour l'ensemble complet d'entra√Ænement\n",
        "    total_loss = running_loss / num_batches\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    print('')\n",
        "    print('√©poque =', epoch, '\\t temps =', elapsed, '\\t lr =', my_lr, '\\t exp(loss) =', math.exp(total_loss))\n",
        "\n",
        "    eval_on_test_set()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "JRu41IdwBbVt",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "### Choisir une phrase (prise de l'ensemble de test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSmC_QyMBbVt"
      },
      "outputs": [],
      "source": [
        "sentence1 = \"some analysts expect oil prices to remain relatively\"\n",
        "\n",
        "sentence2 = \"over the next days and weeks they say investors should look for stocks to\"\n",
        "\n",
        "sentence3 = \"prices averaging roughly $ N a barrel higher in the third\"\n",
        "\n",
        "sentence4 = \"i think my line has been very consistent mrs. hills said at a news\"\n",
        "\n",
        "sentence5 = \"this appears particularly true at gm which had strong sales in\"\n",
        "\n",
        "# ou cr√©ez votre propre phrase. Pas de lettres majuscules ni de ponctuation autoris√©es. Chaque mot doit appartenir au vocabulaire autoris√©.\n",
        "sentence6 = \"be your self,the world\"\n",
        "\n",
        "# CHOISIR LA PHRASE ICI\n",
        "mysentence = sentence5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoC9H2zBBbVu"
      },
      "source": [
        "### Afficher la pr√©diction du r√©seau pour le mot suivant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YenIJTXcBbVu",
        "outputId": "1d8e94f8-5307-45d6-8677-8b8a0bbad341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this appears particularly true at gm which had strong sales in ... \n",
            "\n",
            "25.8%\t the\n",
            "7.3%\t N\n",
            "4.7%\t a\n",
            "2.9%\t <unk>\n",
            "2.6%\t new\n",
            "2.1%\t its\n",
            "1.8%\t august\n",
            "1.4%\t july\n",
            "1.1%\t this\n",
            "1.0%\t an\n",
            "0.9%\t london\n",
            "0.9%\t recent\n",
            "0.7%\t april\n",
            "0.7%\t september\n",
            "0.6%\t january\n",
            "0.6%\t early\n",
            "0.6%\t europe\n",
            "0.5%\t his\n",
            "0.5%\t their\n",
            "0.5%\t cash\n",
            "0.4%\t late\n",
            "0.4%\t which\n",
            "0.4%\t california\n",
            "0.4%\t march\n",
            "0.4%\t chicago\n",
            "0.4%\t <eos>\n",
            "0.4%\t connection\n",
            "0.4%\t u.s.\n",
            "0.3%\t japan\n",
            "0.3%\t part\n"
          ]
        }
      ],
      "source": [
        "minibatch_data = sentence2vector(mysentence)\n",
        "minibatch_data = torch.cat((minibatch_data, minibatch_data), dim=0)  # dupliquer la s√©quence de test pour utiliser la m√™me taille de fen√™tre d'attention pour chaque mot\n",
        "pos = generate_positional_encoding(minibatch_data.size(0), hidden_size)\n",
        "\n",
        "minibatch_data = minibatch_data.to(device)\n",
        "pos = pos.to(device)\n",
        "\n",
        "net.eval()\n",
        "scores = net(minibatch_data, pos)\n",
        "scores = scores[-1, :]  # s√©lectionner le dernier vecteur de score pour la pr√©diction du mot suivant √† partir de la s√©quence d'entr√©e\n",
        "scores = scores[0].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "print(mysentence, '... \\n')\n",
        "show_next_word(scores)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}